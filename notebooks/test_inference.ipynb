{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136cb598-d9e9-41b1-a797-da900c7eae8b",
   "metadata": {},
   "source": [
    "- Greedy decoding with beam search for the 13B works better.\n",
    "- The chat models, e.g. [llama2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), works visibly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b9119df29e78e2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastchat.model import load_model, get_conversation_template\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963b123f-9261-4c84-aae0-9b8ff468b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  6 21:35:15 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:30:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              35W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              20W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "PyTorch version: 2.1.0\n",
      "CUDA version: 11.8\n",
      "#GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"#GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4effc307fa49cefc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# MODEL = \"lmsys/vicuna-13b-v1.5\"\n",
    "# MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "NUM_GPUS = 1\n",
    "CONVERSATIONAL = False\n",
    "\n",
    "PROMPT = \"\"\"You are an expert judge of a content. You'll be given a question, some context related to the question, ground-truth answers, and a candidate that you will judge.\n",
    "\n",
    "Question: what is the name of the compound p4010?\n",
    "Answer: \"Phosphorus pentoxide\"\n",
    "Candidate: Unknown.\n",
    "\n",
    "Is candidate correct?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1167d2fcdcd9f546",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gen(prompt: str, model, tokenizer, max_new_tokens=256, do_sample=True, num_beams=1, top_p=0.9, num_returns=1):\n",
    "    # Run inference\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=do_sample,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=num_returns,\n",
    "            num_beams=num_beams,\n",
    "        )\n",
    "\n",
    "    texts = []\n",
    "    for n in range(output_ids.shape[0]):\n",
    "        texts.append(tokenizer.decode(output_ids[n, inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip())\n",
    "\n",
    "    if num_returns == 1:\n",
    "        return texts[0]\n",
    "    else:\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b35947d2d54fb",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# FastChat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263668e9-0a3a-4312-b05c-9a1009dca30a",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738d031709e24547a59728fd061a1b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8416703153a344b4b44eaa43d4a34266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f944b00747fe46f5882515b66e427ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0846e09ff400473d9d7dbdd69d45d5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63f61c089ee4ceb9114d14b7aefa263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bac46c3b4494229b047c2fa4525e9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fd27ee208c4f059942fa4837bd552b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u3/ekamallo/.conda/envs/llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/u3/ekamallo/.conda/envs/llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m PROMPT\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc_tokenizer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mgen\u001b[0;34m(prompt, model, tokenizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m      7\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m      9\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m input_lengths \u001b[38;5;241m=\u001b[39m (inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43moutput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "fc_model, fc_tokenizer = load_model(\n",
    "    MODEL,\n",
    "    device=\"cuda\",\n",
    "    num_gpus=NUM_GPUS,\n",
    "    max_gpu_memory=None,\n",
    "    load_8bit=False,\n",
    "    cpu_offloading=False,\n",
    "    revision=\"main\",\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7153a-83cb-4e05-bd04-6d55411e0123",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0834c9db-b161-4fd6-b809-239e880e661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your answer as a simple \"yes\" or \"no\".\n"
     ]
    }
   ],
   "source": [
    "fc_model.eval()\n",
    "\n",
    "if CONVERSATIONAL:\n",
    "    conv = get_conversation_template(MODEL)\n",
    "    conv.append_message(conv.roles[0], PROMPT)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    print(prompt)\n",
    "else:\n",
    "    prompt = PROMPT\n",
    "    \n",
    "print(gen(prompt, fc_model, fc_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd07767a9f8528",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b64ce-0c8d-4c57-b697-ac1dc302b6af",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e75c777809008a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6c08628c8d4866abcc61ac41941fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c5e06e4c5542f4b43ac93b33c18f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5a979d3a934c4ab971101eb1aadf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10751ff105254269bbb97d391a00fa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd8cc11c29a4b2cb1593ab47af8ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee019ee9fb4af5864d109a45d8192d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e7056252b845ee86baf1e1726234c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f12f09ce47e49feb51149a224e51946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a70c5ab7c44fa6bd9ab256813f9103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b1e1b1ff40443eae43ee528de63379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110fed05c8254317ac6c34e48f6f448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "config = AutoConfig.from_pretrained(MODEL, return_dict=True)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(MODEL, config=config, device_map=\"auto\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c0e13-52ff-4e77-89db-9a6356624a37",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d1a8c4-bd9c-41f7-8583-59026944d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.\n",
      "\n",
      "Candidate: P4010 could be the molecular formula for some unknown compound.\n",
      "\n",
      "Is candidate correct?\n",
      "Yes, but incorrect.\n",
      "\n",
      "Candidate: P4010 could be an abbreviation for a chemical formula that is not the molecular formula.\n",
      "\n",
      "Is candidate correct?\n",
      "Yes, but incorrect.\n",
      "\n",
      "Candidate: The answer is not \"Phosphorus pentoxide\".\n",
      "\n",
      "Is candidate correct?\n",
      "No.\n",
      "\n",
      "What is the name of the compound p4010?\n",
      "Answer: \"Phosphorus pentoxide\"\n",
      "==============================\n",
      "A: False\n",
      "==============================\n",
      "I have no clue what the candidate is referring to. The only thing that comes to mind is that \"p4010\" is not a chemical compound.\n",
      "\n",
      "Therefore, the candidate is incorrect.\n",
      "\n",
      "Is candidate's answer correct?\n",
      "\n",
      "Yes, the candidate's answer is correct. \"Phosphorus pentoxide\" is the name of the chemical compound with the formula P2O5.\n",
      "==============================\n",
      "Candidate is incorrect. The name of the compound p4010 is \"Phosphorus pentoxide\".\n",
      "\n",
      "Is candidate knowledgeable?\n",
      "\n",
      "It is impossible to determine the candidate's knowledge based on the given information.\n",
      "==============================\n",
      "Answer: No. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No.\n",
      "\n",
      "Is candidate close?\n",
      "No.\n",
      "==============================\n",
      "Answer: Yes\n",
      "==============================\n",
      "Answer: No. The candidate does not know the correct answer to the question.\n",
      "==============================\n",
      "A: False\n",
      "==============================\n",
      "Incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "hf_model.eval()\n",
    "\n",
    "if CONVERSATIONAL:\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": PROMPT},\n",
    "    ]\n",
    "    hf_tokenizer.use_default_system_prompt = False\n",
    "    prompt = hf_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    print(prompt)\n",
    "    print(\"***\" * 20)\n",
    "else:\n",
    "    prompt = PROMPT\n",
    "for text in gen(prompt, hf_model, hf_tokenizer, num_returns=10):\n",
    "    print(text)\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a029473-2319-4878-8052-3a87a4c6ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, candidate is not correct.\n",
      "\n",
      "Why is candidate incorrect?\n",
      "The candidate is incorrect because the compound p4010 is not a known compound. There is no known compound that has the chemical formula p4010. The compound with the chemical formula PO4 is known as phosphorus pentoxide, which is not the same as p4010.\n",
      "==============================\n",
      "Candidate: \"Phosphorus pentoxide\"\n",
      "\n",
      "Yes, the candidate is correct.\n",
      "==============================\n",
      "Candidate: No, the correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "Answer: No, the candidate is incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "A: No, the candidate is incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "Answer: False\n",
      "==============================\n",
      "Candidate is not correct. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No, candidate is not correct. The compound p4010 is phosphorus pentoxide.\n",
      "==============================\n",
      "Candidate: Yes.\n",
      "\n",
      "Is candidate complete?\n",
      "\n",
      "Candidate: No.\n",
      "\n",
      "What is the compound name of p4010?\n",
      "\n",
      "Answer: \"Phosphorus pentoxide\"\n",
      "Candidate: \"Phosphorus pentoxide\"\n",
      "==============================\n",
      "No.\n",
      "\n",
      "What is the correct answer?\n",
      "Phosphorus pentoxide.\n",
      "\n",
      "What is the name of the compound p4010?\n",
      "Phosphorus pentoxide.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "hf_model.eval()\n",
    "\n",
    "if CONVERSATIONAL:\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": PROMPT},\n",
    "    ]\n",
    "    hf_tokenizer.use_default_system_prompt = False\n",
    "    prompt = hf_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    print(prompt)\n",
    "else:\n",
    "    prompt = PROMPT\n",
    "for text in gen(prompt, hf_model, hf_tokenizer, top_p=0.6, num_returns=10):\n",
    "    print(text)\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c623cf4-3ad3-4a16-b68c-6efeae1d4efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: False\n",
      "==============================\n",
      "Answer: False\n",
      "==============================\n",
      "A: False\n",
      "==============================\n",
      "Answer: No, the candidate is incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No, the candidate is incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "Candidate is incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "Answer: No. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No, the candidate is not correct. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No, the candidate is incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No, candidate is not correct. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "hf_model.eval()\n",
    "\n",
    "if CONVERSATIONAL:\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": PROMPT},\n",
    "    ]\n",
    "    hf_tokenizer.use_default_system_prompt = False\n",
    "    prompt = hf_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    print(prompt)\n",
    "else:\n",
    "    prompt = PROMPT\n",
    "    \n",
    "for text in gen(prompt, hf_model, hf_tokenizer, do_sample=False, num_beams=10, num_returns=10):\n",
    "    print(text)\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcf2aaca-3858-4c45-94fe-a4d34c651914",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mistral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[43mget_conversation_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m conv\u001b[38;5;241m.\u001b[39mappend_message(conv\u001b[38;5;241m.\u001b[39mroles[\u001b[38;5;241m0\u001b[39m], PROMPT)\n\u001b[1;32m      3\u001b[0m conv\u001b[38;5;241m.\u001b[39mappend_message(conv\u001b[38;5;241m.\u001b[39mroles[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/QA-eval/src/FastChat/fastchat/model/model_adapter.py:334\u001b[0m, in \u001b[0;36mget_conversation_template\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default conversation template.\"\"\"\u001b[39;00m\n\u001b[1;32m    333\u001b[0m adapter \u001b[38;5;241m=\u001b[39m get_model_adapter(model_path)\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_conv_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/QA-eval/src/FastChat/fastchat/model/model_adapter.py:1313\u001b[0m, in \u001b[0;36mMistralAdapter.get_default_conv_template\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default_conv_template\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Conversation:\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_conv_template\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/QA-eval/src/FastChat/fastchat/conversation.py:297\u001b[0m, in \u001b[0;36mget_conv_template\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_conv_template\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Conversation:\n\u001b[1;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a conversation template.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_templates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mistral'"
     ]
    }
   ],
   "source": [
    "conv = get_conversation_template(MODEL)\n",
    "conv.append_message(conv.roles[0], PROMPT)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "print(conv.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f353d5-b06c-4eb7-9dd9-13fa5af9e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
