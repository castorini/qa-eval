{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b9119df29e78e2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963b123f-9261-4c84-aae0-9b8ff468b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  7 22:30:40 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   30C    P5              32W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:40:00.0 Off |                  Off |\n",
      "| 30%   33C    P8              20W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "PyTorch version: 2.1.0\n",
      "CUDA version: 11.8\n",
      "#GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"#GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4effc307fa49cefc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# MODEL = \"lmsys/vicuna-13b-v1.5\"\n",
    "# MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "# MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "CONVERSATIONAL = True\n",
    "\n",
    "PROMPT_FILE = \"../prompts/eval-v0.2-few-shot_chat.txt\"\n",
    "# PROMPT_FILE = \"../prompts/eval-v0.2-zero-shot_chat.txt\"\n",
    "\n",
    "prompt_template = \"\".join(open(PROMPT_FILE).readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1167d2fcdcd9f546",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gen(example, model, tokenizer, max_new_tokens=256, do_sample=True, num_beams=1, top_p=0.9, num_returns=1):\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(example, dict):\n",
    "        prompt = prepare(example, model, tokenizer)\n",
    "    else:\n",
    "        prompt = example\n",
    "    \n",
    "    # Run inference\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=do_sample,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=num_returns,\n",
    "            num_beams=num_beams,\n",
    "        )\n",
    "\n",
    "    texts = []\n",
    "    for n in range(output_ids.shape[0]):\n",
    "        texts.append(tokenizer.decode(output_ids[n, inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip())\n",
    "\n",
    "    if num_returns == 1:\n",
    "        return texts[0]\n",
    "    else:\n",
    "        return texts\n",
    "    \n",
    "\n",
    "def prepare(example, model, tokenizer, raw_chat=False):\n",
    "    gold_answers = \", \".join([f'\"{a}\"' for a in example[\"answers\"]])\n",
    "    text = prompt_template.format(q=example[\"question\"], answers=gold_answers, candidate_answer=example[\"candidate\"])\n",
    "    \n",
    "    if CONVERSATIONAL:\n",
    "        if \"Mistral\" in model.config.name_or_path:\n",
    "            instructions = None\n",
    "            prompt = text\n",
    "        else:\n",
    "            sections = text.split(\"###\")\n",
    "            instructions = \"###\".join(sections[:-1]) if len(sections) > 1 else None\n",
    "            prompt = sections[-1].strip()\n",
    "        \n",
    "        chat = []\n",
    "        if instructions:\n",
    "            chat.append({\"role\": \"system\", \"content\": instructions})\n",
    "\n",
    "        chat.append({\"role\": \"user\", \"content\": prompt})\n",
    "        if raw_chat:\n",
    "            return chat\n",
    "        else:\n",
    "            return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b64ce-0c8d-4c57-b697-ac1dc302b6af",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e75c777809008a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382cb26d46d04629991d688e05d1f71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL, return_dict=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, config=config, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c0e13-52ff-4e77-89db-9a6356624a37",
   "metadata": {},
   "source": [
    "## Demo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d1a8c4-bd9c-41f7-8583-59026944d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: The cost to make Grand Theft Auto V (GTA V) is estimated to be around $100 million. Therefore, the candidate answer is correct.\n",
      "Judgment: yes.\n",
      "==============================\n",
      "Explanation: The ground-truth answer is $137 million. The candidate answer is approximately $100 million which is close but not exact.\n",
      "\n",
      "Judgment: no.\n",
      "==============================\n",
      "Explanation: The cost of making Grand Theft Auto V (GTA V) was $137 million. However, the candidate answer says approximately $100 million which is less than the ground-truth answer. Therefore, the candidate answer is incorrect.\n",
      "\n",
      "Judgment: no.\n",
      "==============================\n",
      "The ground-truth answer for the question \"how much money did it cost to make GTA V\" is \"137\", while the candidate answer is \"approximately $100 million\". The candidate answer is correct based on the fact that $100 million is approximately the same as 137 in the context of money, given that each unit of money represents a significant amount. Therefore, the judgment is \"yes\".\n",
      "==============================\n",
      "Explanation: The actual cost to make GTA V was $137 million as per the ground-truth answer. However, the candidate answer provides an approximate value of $100 million which can be considered close enough and acceptable in general terms.\n",
      "\n",
      "Judgment: yes.\n",
      "==============================\n",
      "The candidate answer is incorrect as the ground-truth answer is $137 million.\n",
      "==============================\n",
      "The ground-truth answer for how much money it cost to make GTA V is $137 million. The candidate answer of approximately $100 million is incorrect, so the judgment is no.\n",
      "==============================\n",
      "The actual cost of developing Grand Theft Auto V (GTA V) was $137 million, which is less than the candidate answer of \"approximately $100 million\". Therefore, the candidate answer cannot be considered correct and the judgment is no.\n",
      "==============================\n",
      "The cost to make Grand Theft Auto V is estimated to be around $100 million. However, this figure is an estimate, and the actual cost could be slightly more or less. Therefore, the candidate answer is approximately correct.\n",
      "\n",
      "Judgment: yes.\n",
      "==============================\n",
      "The ground-truth answer for the cost of making Grand Theft Auto V is $137 million. However, the candidate answer is based on an approximation of $100 million, which is not entirely incorrect, but not the exact amount. Therefore, the candidate answer is not fully correct.\n",
      "\n",
      "Judgment: no.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"question\": \"how much money did it cost to make gta v?\",\n",
    "    \"answers\": ['137'],\n",
    "    \"candidate\": \"approximately $100 million\",\n",
    "}\n",
    "\n",
    "responses = []\n",
    "for text in gen(example, model, tokenizer, num_returns=10):\n",
    "    responses.append(text)\n",
    "    print(text)\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e892ae6-9a61-4eb5-ae37-5ea08f3d684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Explanation: The cost to make Grand Theft Auto V (GTA V) is estimated to be around $100 million. Therefore, the candidate answer is correct.\n",
      "Judgment: yes.\n",
      ">>>> yes\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Explanation: The ground-truth answer is $137 million. The candidate answer is approximately $100 million which is close but not exact.\n",
      "\n",
      "Judgment: no.\n",
      ">>>> no\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Explanation: The cost of making Grand Theft Auto V (GTA V) was $137 million. However, the candidate answer says approximately $100 million which is less than the ground-truth answer. Therefore, the candidate answer is incorrect.\n",
      "\n",
      "Judgment: no.\n",
      ">>>> no\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> The ground-truth answer for the question \"how much money did it cost to make GTA V\" is \"137\", while the candidate answer is \"approximately $100 million\". The candidate answer is correct based on the fact that $100 million is approximately the same as 137 in the context of money, given that each unit of money represents a significant amount. Therefore, the judgment is \"yes\".\n",
      ">>>> Yes.\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Explanation: The actual cost to make GTA V was $137 million as per the ground-truth answer. However, the candidate answer provides an approximate value of $100 million which can be considered close enough and acceptable in general terms.\n",
      "\n",
      "Judgment: yes.\n",
      ">>>> yes\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> The candidate answer is incorrect as the ground-truth answer is $137 million.\n",
      ">>>> no\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> The ground-truth answer for how much money it cost to make GTA V is $137 million. The candidate answer of approximately $100 million is incorrect, so the judgment is no.\n",
      ">>>> no\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> The actual cost of developing Grand Theft Auto V (GTA V) was $137 million, which is less than the candidate answer of \"approximately $100 million\". Therefore, the candidate answer cannot be considered correct and the judgment is no.\n",
      ">>>> no\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> The cost to make Grand Theft Auto V is estimated to be around $100 million. However, this figure is an estimate, and the actual cost could be slightly more or less. Therefore, the candidate answer is approximately correct.\n",
      "\n",
      "Judgment: yes.\n",
      ">>>> yes\n",
      "==============================\n",
      ">> The ground-truth answer for the cost of making Grand Theft Auto V is $137 million. However, the candidate answer is based on an approximation of $100 million, which is not entirely incorrect, but not the exact amount. Therefore, the candidate answer is not fully correct.\n",
      "\n",
      "Judgment: no.\n",
      ">>>> no\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "chat = prepare(example, model, tokenizer, raw_chat=True)\n",
    "\n",
    "next_responses = []\n",
    "for response in responses:\n",
    "    c = list(chat)\n",
    "    c.append({\"role\": \"assistant\", \"content\": response})\n",
    "    c.append({\"role\": \"user\", \"content\": \"Tell me your final judgment in only 'yes' or 'no'\"})\n",
    "    conv = tokenizer.apply_chat_template(c, tokenize=False, add_generation_prompt=True)\n",
    "    out = gen(conv, model, tokenizer, do_sample=False)\n",
    "    print(\">>\", response)\n",
    "    print(\">>>>\", out)\n",
    "    print(\"===\" * 10)\n",
    "    next_responses.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cb9b8-8b58-479b-b684-cd057731ca3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
