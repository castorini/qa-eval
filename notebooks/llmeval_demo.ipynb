{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b9119df29e78e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963b123f-9261-4c84-aae0-9b8ff468b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  6 21:35:15 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:30:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              35W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              20W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "PyTorch version: 2.1.0\n",
      "CUDA version: 11.8\n",
      "#GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"#GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4effc307fa49cefc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL = \"lmsys/vicuna-13b-v1.5\"\n",
    "# MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "CONVERSATIONAL = True\n",
    "\n",
    "PROMPT_FILE = \"../prompts/eval-v0.2-few-shot_chat.txt\"\n",
    "# PROMPT_FILE = \"../prompts/eval-v0.2-zero-shot_chat.txt\"\n",
    "\n",
    "prompt_template = \"\".join(open(PROMPT_FILE).readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1167d2fcdcd9f546",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen(example, model, tokenizer, max_new_tokens=256, do_sample=True, num_beams=1, top_p=0.9, num_returns=1):\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(example, dict):\n",
    "        prompt = prepare(example, model)\n",
    "    else:\n",
    "        prompt = example\n",
    "    \n",
    "    # Run inference\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=do_sample,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=num_returns,\n",
    "            num_beams=num_beams,\n",
    "        )\n",
    "\n",
    "    texts = []\n",
    "    for n in range(output_ids.shape[0]):\n",
    "        texts.append(tokenizer.decode(output_ids[n, inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip())\n",
    "\n",
    "    if num_returns == 1:\n",
    "        return texts[0]\n",
    "    else:\n",
    "        return texts\n",
    "    \n",
    "\n",
    "def prepare(example, model):\n",
    "    gold_answers = \", \".join([f'\"{a}\"' for a in example[\"answers\"]])\n",
    "    text = prompt_template.format(q=example[\"question\"], answers=gold_answers, candidate_answer=example[\"candidate\"])\n",
    "    \n",
    "    if CONVERSATIONAL:\n",
    "        if \"Mistral\" in model.config.name_or_path:\n",
    "            instructions = None\n",
    "            prompt = text\n",
    "        else:\n",
    "            sections = text.split(\"###\")\n",
    "            instructions = \"###\".join(sections[:-1]) if len(sections) > 1 else None\n",
    "            prompt = sections[-1].strip()\n",
    "        \n",
    "        chat = []\n",
    "        if instructions:\n",
    "            chat.append({\"role\": \"system\", \"content\": instructions})\n",
    "\n",
    "        chat.append({\"role\": \"user\", \"content\": prompt})\n",
    "        return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b64ce-0c8d-4c57-b697-ac1dc302b6af",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e75c777809008a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6c08628c8d4866abcc61ac41941fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c5e06e4c5542f4b43ac93b33c18f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5a979d3a934c4ab971101eb1aadf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10751ff105254269bbb97d391a00fa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd8cc11c29a4b2cb1593ab47af8ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee019ee9fb4af5864d109a45d8192d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e7056252b845ee86baf1e1726234c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f12f09ce47e49feb51149a224e51946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a70c5ab7c44fa6bd9ab256813f9103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b1e1b1ff40443eae43ee528de63379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110fed05c8254317ac6c34e48f6f448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL, return_dict=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, config=config, device_map=\"auto\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c0e13-52ff-4e77-89db-9a6356624a37",
   "metadata": {},
   "source": [
    "## Demo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d1a8c4-bd9c-41f7-8583-59026944d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.\n",
      "\n",
      "Candidate: P4010 could be the molecular formula for some unknown compound.\n",
      "\n",
      "Is candidate correct?\n",
      "Yes, but incorrect.\n",
      "\n",
      "Candidate: P4010 could be an abbreviation for a chemical formula that is not the molecular formula.\n",
      "\n",
      "Is candidate correct?\n",
      "Yes, but incorrect.\n",
      "\n",
      "Candidate: The answer is not \"Phosphorus pentoxide\".\n",
      "\n",
      "Is candidate correct?\n",
      "No.\n",
      "\n",
      "What is the name of the compound p4010?\n",
      "Answer: \"Phosphorus pentoxide\"\n",
      "==============================\n",
      "A: False\n",
      "==============================\n",
      "I have no clue what the candidate is referring to. The only thing that comes to mind is that \"p4010\" is not a chemical compound.\n",
      "\n",
      "Therefore, the candidate is incorrect.\n",
      "\n",
      "Is candidate's answer correct?\n",
      "\n",
      "Yes, the candidate's answer is correct. \"Phosphorus pentoxide\" is the name of the chemical compound with the formula P2O5.\n",
      "==============================\n",
      "Candidate is incorrect. The name of the compound p4010 is \"Phosphorus pentoxide\".\n",
      "\n",
      "Is candidate knowledgeable?\n",
      "\n",
      "It is impossible to determine the candidate's knowledge based on the given information.\n",
      "==============================\n",
      "Answer: No. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n",
      "No.\n",
      "\n",
      "Is candidate close?\n",
      "No.\n",
      "==============================\n",
      "Answer: Yes\n",
      "==============================\n",
      "Answer: No. The candidate does not know the correct answer to the question.\n",
      "==============================\n",
      "A: False\n",
      "==============================\n",
      "Incorrect. The correct answer is \"Phosphorus pentoxide\".\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"question\": \"where are the highest average incomes found in north america?\",\n",
    "    \"answers\": ['Virginia'],\n",
    "    \"candidate\": \"Canada\",\n",
    "}\n",
    "\n",
    "for text in gen(example, model, tokenizer, num_returns=10):\n",
    "    print(text)\n",
    "    print(\"===\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
